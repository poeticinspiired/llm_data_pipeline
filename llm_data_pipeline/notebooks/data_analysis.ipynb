{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Training Data Curation Pipeline: Data Analysis\n",
    "\n",
    "This notebook demonstrates the analysis of legal text data processed through our LLM Training Data Curation Pipeline. We'll explore the data quality, distribution, and characteristics that make it suitable for language model training.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Load and explore processed legal documents from our MongoDB database\n",
    "2. Analyze data quality metrics and distribution\n",
    "3. Examine text characteristics relevant for LLM training\n",
    "4. Visualize key insights about the dataset\n",
    "5. Generate sample statistics for model training considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Add the project root to the Python path\n",
    "project_root = Path(\"..\")\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import our pipeline modules\n",
    "from src.data_processing.base import ProcessedDocument\n",
    "from src.data_storage.mongodb import MongoDBStorage\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB and Load Data\n",
    "\n",
    "Now, let's connect to our MongoDB database and load the processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# MongoDB connection parameters\n",
    "MONGODB_CONNECTION_STRING = \"mongodb://localhost:27017/\"\n",
    "MONGODB_DATABASE = \"llm_data_pipeline\"\n",
    "MONGODB_COLLECTION = \"processed_documents\"\n",
    "\n",
    "# Create MongoDB storage instance\n",
    "mongodb_storage = MongoDBStorage(\n",
    "    connection_string=MONGODB_CONNECTION_STRING,\n",
    "    database_name=MONGODB_DATABASE,\n",
    "    collection_name=MONGODB_COLLECTION\n",
    ")\n",
    "\n",
    "# Connect to MongoDB\n",
    "if mongodb_storage.connect():\n",
    "    print(f\"Connected to MongoDB: {MONGODB_DATABASE}.{MONGODB_COLLECTION}\")\n",
    "else:\n",
    "    print(\"Failed to connect to MongoDB. Using sample data instead.\")\n",
    "    # We'll create sample data later if connection fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to load documents from MongoDB or generate sample data\n",
    "def load_documents(limit=1000, use_sample=False):\n",
    "    if not use_sample and mongodb_storage.collection:\n",
    "        # Query documents from MongoDB\n",
    "        print(f\"Loading up to {limit} documents from MongoDB...\")\n",
    "        documents = mongodb_storage.query_documents({}, limit=limit)\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        return documents\n",
    "    else:\n",
    "        # Generate sample data\n",
    "        print(\"Generating sample data...\")\n",
    "        return generate_sample_documents(count=min(limit, 100))\n",
    "\n",
    "# Function to generate sample documents for demonstration\n",
    "def generate_sample_documents(count=100):\n",
    "    documents = []\n",
    "    \n",
    "    # Sample legal phrases and terms\n",
    "    legal_phrases = [\n",
    "        \"The Court finds that\", \"It is hereby ordered\", \"The plaintiff argues\",\n",
    "        \"The defendant contends\", \"According to precedent\", \"The statute provides\",\n",
    "        \"Under the law\", \"The evidence shows\", \"The jury concluded\", \"In this case\"\n",
    "    ]\n",
    "    \n",
    "    legal_terms = [\n",
    "        \"jurisdiction\", \"tort\", \"liability\", \"damages\", \"plaintiff\", \"defendant\",\n",
    "        \"appeal\", \"motion\", \"injunction\", \"testimony\", \"evidence\", \"ruling\",\n",
    "        \"statute\", \"precedent\", \"contract\", \"negligence\", \"remedy\", \"violation\"\n",
    "    ]\n",
    "    \n",
    "    # Sample court names\n",
    "    courts = [\n",
    "        \"Supreme Court\", \"Circuit Court\", \"District Court\", \"Court of Appeals\",\n",
    "        \"Federal Court\", \"State Court\", \"Municipal Court\", \"Bankruptcy Court\"\n",
    "    ]\n",
    "    \n",
    "    # Generate documents\n",
    "    for i in range(count):\n",
    "        # Generate random text length\n",
    "        text_length = random.randint(5, 20)  # paragraphs\n",
    "        \n",
    "        # Generate random text\n",
    "        paragraphs = []\n",
    "        for _ in range(text_length):\n",
    "            # Generate a paragraph with 3-8 sentences\n",
    "            sentences = []\n",
    "            for _ in range(random.randint(3, 8)):\n",
    "                # Start with a legal phrase\n",
    "                phrase = random.choice(legal_phrases)\n",
    "                \n",
    "                # Add 5-15 random legal terms\n",
    "                terms = random.sample(legal_terms, random.randint(5, 15))\n",
    "                \n",
    "                # Construct a sentence\n",
    "                sentence = f\"{phrase} {' '.join(terms)}.\"\n",
    "                sentences.append(sentence)\n",
    "            \n",
    "            # Join sentences into a paragraph\n",
    "            paragraph = \" \".join(sentences)\n",
    "            paragraphs.append(paragraph)\n",
    "        \n",
    "        # Join paragraphs into a document\n",
    "        text = \"\\n\\n\".join(paragraphs)\n",
    "        \n",
    "        # Create a ProcessedDocument\n",
    "        doc = ProcessedDocument(\n",
    "            id=f\"sample-{i+1}\",\n",
    "            source=\"sample\",\n",
    "            source_id=f\"sample-{i+1}\",\n",
    "            text=text\n",
    "        )\n",
    "        \n",
    "        # Add random tokens\n",
    "        doc.tokens = text.split()\n",
    "        doc.token_count = len(doc.tokens)\n",
    "        \n",
    "        # Add random quality score and metrics\n",
    "        doc.quality_score = random.uniform(0.5, 1.0)\n",
    "        doc.quality_metrics = {\n",
    "            \"text_length\": len(text),\n",
    "            \"word_count\": len(text.split()),\n",
    "            \"avg_word_length\": sum(len(word) for word in text.split()) / max(1, len(text.split())),\n",
    "            \"sentence_count\": sum(1 for _ in text.split(\".\") if _),\n",
    "            \"alphanumeric_ratio\": sum(1 for c in text if c.isalnum()) / max(1, len(text))\n",
    "        }\n",
    "        \n",
    "        # Add metadata\n",
    "        doc.original_metadata = {\n",
    "            \"court\": random.choice(courts),\n",
    "            \"year\": random.randint(2000, 2023),\n",
    "            \"case_type\": random.choice([\"Civil\", \"Criminal\", \"Administrative\", \"Constitutional\"])\n",
    "        }\n",
    "        \n",
    "        # Add processing metadata\n",
    "        doc.processing_metadata = {\n",
    "            \"filtered\": False,\n",
    "            \"duplicate\": False,\n",
    "            \"sentence_count\": doc.quality_metrics[\"sentence_count\"],\n",
    "            \"sentences\": text.split(\".\")\n",
    "        }\n",
    "        \n",
    "        # Add to documents list\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load documents\n",
    "documents = load_documents(limit=1000)\n",
    "\n",
    "# If no documents were loaded, use sample data\n",
    "if not documents:\n",
    "    documents = load_documents(limit=100, use_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to DataFrame for Analysis\n",
    "\n",
    "Let's convert our documents to a pandas DataFrame for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def documents_to_dataframe(documents):\n",
    "    \"\"\"Convert a list of ProcessedDocument objects to a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Extract basic document info\n",
    "        doc_dict = {\n",
    "            \"id\": doc.id,\n",
    "            \"source\": doc.source,\n",
    "            \"text_length\": len(doc.text),\n",
    "            \"token_count\": doc.token_count,\n",
    "            \"quality_score\": doc.quality_score\n",
    "        }\n",
    "        \n",
    "        # Extract quality metrics\n",
    "        if doc.quality_metrics:\n",
    "            for key, value in doc.quality_metrics.items():\n",
    "                if isinstance(value, (int, float, str, bool)):\n",
    "                    doc_dict[f\"quality_{key}\"] = value\n",
    "        \n",
    "        # Extract original metadata\n",
    "        if doc.original_metadata:\n",
    "            for key, value in doc.original_metadata.items():\n",
    "                if isinstance(value, (int, float, str, bool)):\n",
    "                    doc_dict[f\"metadata_{key}\"] = value\n",
    "        \n",
    "        # Extract processing metadata\n",
    "        if doc.processing_metadata:\n",
    "            for key, value in doc.processing_metadata.items():\n",
    "                if isinstance(value, (int, float, str, bool)):\n",
    "                    doc_dict[f\"processing_{key}\"] = value\n",
    "        \n",
    "        data.append(doc_dict)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Convert documents to DataFrame\n",
    "df = documents_to_dataframe(documents)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis\n",
    "\n",
    "Let's analyze the quality of our dataset based on various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "print(\"Basic statistics for numerical columns:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quality score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['quality_score'], kde=True, bins=20)\n",
    "plt.title('Distribution of Quality Scores')\n",
    "plt.xlabel('Quality Score')\n",
    "plt.ylabel('Count')\n",
    "plt.axvline(x=0.7, color='r', linestyle='--', label='Minimum Threshold (0.7)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Document length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['text_length'], kde=True, bins=20)\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Token count distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['token_count'], kde=True, bins=20)\n",
    "plt.title('Distribution of Token Counts')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quality metrics correlation\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "quality_cols = [col for col in numeric_cols if col.startswith('quality_') or col in ['quality_score', 'text_length', 'token_count']]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[quality_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Between Quality Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Characteristics Analysis\n",
    "\n",
    "Let's analyze the characteristics of the text data that are relevant for LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to extract and analyze tokens from documents\n",
    "def analyze_tokens(documents, max_docs=100):\n",
    "    \"\"\"Analyze tokens from a list of documents.\"\"\"\n",
    "    # Limit to max_docs for performance\n",
    "    docs_to_analyze = documents[:max_docs]\n",
    "    \n",
    "    # Collect all tokens\n",
    "    all_tokens = []\n",
    "    for doc in docs_to_analyze:\n",
    "        if doc.tokens:\n",
    "            all_tokens.extend(doc.tokens)\n",
    "    \n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Calculate vocabulary size\n",
    "    vocab_size = len(token_counts)\n",
    "    \n",
    "    # Get most common tokens\n",
    "    most_common = token_counts.most_common(30)\n",
    "    \n",
    "    # Calculate token length distribution\n",
    "    token_lengths = [len(token) for token in all_tokens]\n",
    "    \n",
    "    return {\n",
    "        \"total_tokens\": len(all_tokens),\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"most_common\": most_common,\n",
    "        \"token_counts\": token_counts,\n",
    "        \"token_lengths\": token_lengths\n",
    "    }\n",
    "\n",
    "# Analyze tokens\n",
    "token_analysis = analyze_tokens(documents)\n",
    "\n",
    "print(f\"Total tokens analyzed: {token_analysis['total_tokens']}\")\n",
    "print(f\"Vocabulary size: {token_analysis['vocab_size']}\")\n",
    "print(f\"Vocabulary coverage: {token_analysis['vocab_size'] / max(1, token_analysis['total_tokens']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot most common tokens\n",
    "plt.figure(figsize=(12, 8))\n",
    "most_common_df = pd.DataFrame(token_analysis['most_common'], columns=['Token', 'Count'])\n",
    "sns.barplot(x='Count', y='Token', data=most_common_df)\n",
    "plt.title('30 Most Common Tokens')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Token')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot token length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(token_analysis['token_lengths'], kde=True, bins=20)\n",
    "plt.title('Distribution of Token Lengths')\n",
    "plt.xlabel('Token Length (characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate_from_frequencies(token_analysis['token_counts'])\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Analysis\n",
    "\n",
    "Let's analyze the metadata associated with our documents to understand the composition of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract metadata columns\n",
    "metadata_cols = [col for col in df.columns if col.startswith('metadata_')]\n",
    "\n",
    "if metadata_cols:\n",
    "    print(f\"Found {len(metadata_cols)} metadata columns: {metadata_cols}\")\n",
    "    \n",
    "    # Analyze categorical metadata\n",
    "    for col in metadata_cols:\n",
    "        if df[col].dtype == 'object' or len(df[col].unique()) < 20:  # Categorical or few unique values\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts = df[col].value_counts()\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col.replace('metadata_', ''))\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No metadata columns found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Filtering Analysis\n",
    "\n",
    "Let's analyze the filtering results to understand how many documents were filtered out and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if filtering information is available\n",
    "if 'processing_filtered' in df.columns:\n",
    "    # Count filtered documents\n",
    "    filtered_count = df['processing_filtered'].sum()\n",
    "    total_count = len(df)\n",
    "    \n",
    "    print(f\"Filtered documents: {filtered_count} ({filtered_count/total_count:.2%})\")\n",
    "    print(f\"Retained documents: {total_count - filtered_count} ({(total_count - filtered_count)/total_count:.2%})\")\n",
    "    \n",
    "    # Plot filtering results\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie([total_count - filtered_count, filtered_count], \n",
    "            labels=['Retained', 'Filtered'], \n",
    "            autopct='%1.1f%%',\n",
    "            colors=['#66b3ff', '#ff9999'],\n",
    "            explode=(0.1, 0))\n",
    "    plt.title('Document Filtering Results')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if filter reason is available\n",
    "    if 'processing_filter_reason' in df.columns:\n",
    "        # Count filter reasons\n",
    "        filter_reasons = df.loc[df['processing_filtered'], 'processing_filter_reason'].value_counts()\n",
    "        \n",
    "        if not filter_reasons.empty:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x=filter_reasons.values, y=filter_reasons.index)\n",
    "            plt.title('Filter Reasons')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel('Reason')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No filtering information available in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication Analysis\n",
    "\n",
    "Let's analyze the deduplication results to understand how many documents were identified as duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if deduplication information is available\n",
    "if 'processing_duplicate' in df.columns:\n",
    "    # Count duplicate documents\n",
    "    duplicate_count = df['processing_duplicate'].sum()\n",
    "    total_count = len(df)\n",
    "    \n",
    "    print(f\"Duplicate documents: {duplicate_count} ({duplicate_count/total_count:.2%})\")\n",
    "    print(f\"Unique documents: {total_count - duplicate_count} ({(total_count - duplicate_count)/total_count:.2%})\")\n",
    "    \n",
    "    # Plot deduplication results\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie([total_count - duplicate_count, duplicate_count], \n",
    "            labels=['Unique', 'Duplicate'], \n",
    "            autopct='%1.1f%%',\n",
    "            colors=['#66b3ff', '#ff9999'],\n",
    "            explode=(0.1, 0))\n",
    "    plt.title('Document Deduplication Results')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if similarity information is available\n",
    "    if 'processing_similarity' in df.columns:\n",
    "        # Plot similarity distribution for duplicates\n",
    "        similarities = df.loc[df['processing_duplicate'], 'processing_similarity']\n",
    "        \n",
    "        if not similarities.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(similarities, kde=True, bins=20)\n",
    "            plt.title('Similarity Distribution for Duplicates')\n",
    "            plt.xlabel('Similarity')\n",
    "            plt.ylabel('Count')\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No deduplication information available in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Document Analysis\n",
    "\n",
    "Let's examine a few sample documents to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to display a sample document\n",
    "def display_sample_document(doc_index=0):\n",
    "    \"\"\"Display a sample document with its metadata and quality metrics.\"\"\"\n",
    "    if doc_index < 0 or doc_index >= len(documents):\n",
    "        print(f\"Invalid document index. Must be between 0 and {len(documents)-1}.\")\n",
    "        return\n",
    "    \n",
    "    doc = documents[doc_index]\n",
    "    \n",
    "    print(f\"Document ID: {doc.id}\")\n",
    "    print(f\"Source: {doc.source}\")\n",
    "    print(f\"Quality Score: {doc.quality_score:.4f}\")\n",
    "    print(f\"Token Count: {doc.token_count}\")\n",
    "    print(\"\\nOriginal Metadata:\")\n",
    "    for key, value in doc.original_metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nQuality Metrics:\")\n",
    "    for key, value in doc.quality_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nText Sample (first 500 characters):\")\n",
    "    print(doc.text[:500] + \"...\")\n",
    "    \n",
    "    if doc.tokens:\n",
    "        print(\"\\nTokens Sample (first 20):\")\n",
    "        print(doc.tokens[:20])\n",
    "\n",
    "# Display a sample document\n",
    "display_sample_document(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display another sample document with high quality score\n",
    "if len(documents) > 1:\n",
    "    # Find a document with high quality score\n",
    "    high_quality_indices = [i for i, doc in enumerate(documents) if doc.quality_score > 0.8]\n",
    "    if high_quality_indices:\n",
    "        print(\"Sample document with high quality score:\")\n",
    "        display_sample_document(high_quality_indices[0])\n",
    "    else:\n",
    "        print(\"No documents with high quality score found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Training Considerations\n",
    "\n",
    "Based on our analysis, let's discuss some considerations for using this dataset for LLM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate dataset statistics relevant for LLM training\n",
    "total_docs = len(documents)\n",
    "total_tokens = sum(doc.token_count for doc in documents if doc.token_count)\n",
    "avg_tokens_per_doc = total_tokens / total_docs if total_docs > 0 else 0\n",
    "\n",
    "# Estimate training time and resources\n",
    "tokens_per_gpu_day = 5_000_000  # Rough estimate for a single GPU day of training\n",
    "estimated_gpu_days = total_tokens / tokens_per_gpu_day\n",
    "\n",
    "print(\"Dataset Statistics for LLM Training:\")\n",
    "print(f\"Total documents: {total_docs:,}\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Average tokens per document: {avg_tokens_per_doc:.2f}\")\n",
    "print(f\"Estimated GPU days for training: {estimated_gpu_days:.2f}\")\n",
    "\n",
    "# Calculate quality distribution\n",
    "quality_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "quality_counts = [sum(1 for doc in documents if doc.quality_score >= threshold) for threshold in quality_thresholds]\n",
    "quality_percentages = [count / total_docs * 100 for count in quality_counts]\n",
    "\n",
    "# Plot quality threshold impact\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(quality_thresholds, quality_percentages, marker='o', linewidth=2)\n",
    "plt.title('Impact of Quality Threshold on Dataset Size')\n",
    "plt.xlabel('Quality Score Threshold')\n",
    "plt.ylabel('Percentage of Documents Retained')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for LLM Training\n",
    "\n",
    "Based on our analysis, here are some recommendations for using this dataset for LLM training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Quality Filtering**: Apply a quality score threshold of at least 0.7 to ensure high-quality training data.\n",
    "\n",
    "2. **Token Length Considerations**: The average document length is suitable for training, but consider chunking very long documents to improve training efficiency.\n",
    "\n",
    "3. **Vocabulary Coverage**: The vocabulary size is appropriate for a domain-specific legal language model, with good coverage of legal terminology.\n",
    "\n",
    "4. **Data Balancing**: Consider balancing the dataset across different courts and case types to ensure the model learns a diverse range of legal language.\n",
    "\n",
    "5. **Deduplication**: The deduplication process has effectively removed similar documents, ensuring the model doesn't overfit to repeated content.\n",
    "\n",
    "6. **Training Resources**: Based on the total token count, allocate appropriate GPU resources for training, with an estimated requirement of several GPU days for a small to medium-sized model.\n",
    "\n",
    "7. **Evaluation**: Set aside a portion of the dataset (10-15%) for evaluation to measure model performance on legal text understanding and generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've analyzed the legal text dataset processed through our LLM Training Data Curation Pipeline. We've examined data quality, text characteristics, metadata distribution, and filtering results to gain insights into the dataset's suitability for LLM training.\n",
    "\n",
    "The dataset demonstrates good quality overall, with appropriate token distributions and vocabulary coverage for legal domain training. By applying the recommended quality thresholds and preprocessing steps, this dataset can serve as a valuable resource for training a specialized legal language model.\n",
    "\n",
    "The pipeline has successfully curated a dataset that balances quality, diversity, and domain-specificity, making it well-suited for the intended LLM training task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
